{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "691ce24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5d3305e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from datasets import Dataset, Features, Value, ClassLabel, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9b8534fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_NAME_SIZE = 1\n",
    "THRESHOLD = 0.8\n",
    "DATASET_SIZE = 75\n",
    "TEMPLATE_TYPE = \"subject_with_name\"\n",
    "MODEL_NAME =  \"gpt2-small\" # \"facebook/opt-125m\", \n",
    "PROMPT_TYPE_SIZE = 50\n",
    "\n",
    "NAMES_FILEPATH = \"../../datasets/names.csv\"\n",
    "DATASET_PATH = \"../../datasets/names_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "caf60f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(673, 339)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True\n",
    ")\n",
    "\n",
    "she_token = model.to_tokens(f\" she\", prepend_bos=False)[0].tolist()[0]\n",
    "he_token  = model.to_tokens(f\" he\", prepend_bos=False)[0].tolist()[0]\n",
    "\n",
    "she_token, he_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "87f60d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'If', ' I', \"'m\", ' not', ' mistaken', ',', ' Lisa', ' acknowledged', ' Christopher', ' because']\n",
      "Tokenized answer: [' she']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.44</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.54</span><span style=\"font-weight: bold\">% Token: | she|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m16.44\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m28.54\u001b[0m\u001b[1m% Token: | she|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.53 Prob: 31.34% Token: | he|\n",
      "Top 1th token. Logit: 16.44 Prob: 28.54% Token: | she|\n",
      "Top 2th token. Logit: 15.56 Prob: 11.92% Token: | of|\n",
      "Top 3th token. Logit: 14.52 Prob:  4.21% Token: | they|\n",
      "Top 4th token. Logit: 13.79 Prob:  2.02% Token: | \"|\n",
      "Top 5th token. Logit: 13.65 Prob:  1.75% Token: | the|\n",
      "Top 6th token. Logit: 13.57 Prob:  1.62% Token: |,|\n",
      "Top 7th token. Logit: 13.55 Prob:  1.60% Token: | Christopher|\n",
      "Top 8th token. Logit: 13.48 Prob:  1.49% Token: | it|\n",
      "Top 9th token. Logit: 13.34 Prob:  1.29% Token: | his|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' she'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' she'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.reset_hooks(including_permanent=True)\n",
    "# example_prompt = \"The nation heard that Laura had announced how\"\n",
    "# example_prompt = \"If I'm not mistaken, nobody was talking about Lisa because\"\n",
    "example_prompt = \"If I'm not mistaken, Lisa acknowledged Christopher because\"\n",
    "example_answer = \" she\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fc665b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HookedTransformer.from_pretrained(\n",
    "#     \"facebook/opt-125m\",\n",
    "#     center_unembed=True,\n",
    "#     center_writing_weights=True,\n",
    "#     fold_ln=True,\n",
    "#     refactor_factored_attn_matrices=True\n",
    "# )\n",
    "# \n",
    "# she_token = model.to_tokens(f\" she\", prepend_bos=False)[0].tolist()[0]\n",
    "# he_token  = model.to_tokens(f\" he\", prepend_bos=False)[0].tolist()[0]\n",
    "# \n",
    "# she_token, he_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "03790120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>name_weight</th>\n",
       "      <th>F_prop</th>\n",
       "      <th>M_prop</th>\n",
       "      <th>F_weighted</th>\n",
       "      <th>M_weighted</th>\n",
       "      <th>F_weighted_norm</th>\n",
       "      <th>M_weighted_norm</th>\n",
       "      <th>gpt2-small</th>\n",
       "      <th>gpt2-small-size</th>\n",
       "      <th>gpt2-medium</th>\n",
       "      <th>gpt2-medium-size</th>\n",
       "      <th>gpt2-large</th>\n",
       "      <th>gpt2-large-size</th>\n",
       "      <th>facebook/opt-125m</th>\n",
       "      <th>facebook/opt-125m-size</th>\n",
       "      <th>EleutherAI/gpt-neo-125M</th>\n",
       "      <th>EleutherAI/gpt-neo-125M-size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaban</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>2.550345e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.550345e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>317,45094</td>\n",
       "      <td>2</td>\n",
       "      <td>317,45094</td>\n",
       "      <td>2</td>\n",
       "      <td>317,45094</td>\n",
       "      <td>2</td>\n",
       "      <td>83,26528</td>\n",
       "      <td>2</td>\n",
       "      <td>317,45094</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabha</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>8.208008e-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.208008e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>317,397,3099</td>\n",
       "      <td>3</td>\n",
       "      <td>317,397,3099</td>\n",
       "      <td>3</td>\n",
       "      <td>317,397,3099</td>\n",
       "      <td>3</td>\n",
       "      <td>83,873,1999</td>\n",
       "      <td>3</td>\n",
       "      <td>317,397,3099</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aabid</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.465716e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.465716e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>317,397,312</td>\n",
       "      <td>3</td>\n",
       "      <td>317,397,312</td>\n",
       "      <td>3</td>\n",
       "      <td>317,397,312</td>\n",
       "      <td>3</td>\n",
       "      <td>83,873,808</td>\n",
       "      <td>3</td>\n",
       "      <td>317,397,312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aabriella</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4.397147e-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.397147e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>317,397,380,12627</td>\n",
       "      <td>4</td>\n",
       "      <td>317,397,380,12627</td>\n",
       "      <td>4</td>\n",
       "      <td>317,397,380,12627</td>\n",
       "      <td>4</td>\n",
       "      <td>83,873,1069,8461</td>\n",
       "      <td>4</td>\n",
       "      <td>317,397,380,12627</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aada</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.465716e-08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.465716e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>317,4763</td>\n",
       "      <td>2</td>\n",
       "      <td>317,4763</td>\n",
       "      <td>2</td>\n",
       "      <td>317,4763</td>\n",
       "      <td>2</td>\n",
       "      <td>83,2095</td>\n",
       "      <td>2</td>\n",
       "      <td>317,4763</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name   F   M   name_weight  F_prop  M_prop    F_weighted  \\\n",
       "0      Aaban   0  87  2.550345e-07     0.0     1.0  0.000000e+00   \n",
       "1      Aabha  28   0  8.208008e-08     1.0     0.0  8.208008e-08   \n",
       "2      Aabid   0   5  1.465716e-08     0.0     1.0  0.000000e+00   \n",
       "3  Aabriella  15   0  4.397147e-08     1.0     0.0  4.397147e-08   \n",
       "4       Aada   5   0  1.465716e-08     1.0     0.0  1.465716e-08   \n",
       "\n",
       "     M_weighted  F_weighted_norm  M_weighted_norm         gpt2-small  \\\n",
       "0  2.550345e-07         0.000000         0.000505          317,45094   \n",
       "1  0.000000e+00         0.000166         0.000000       317,397,3099   \n",
       "2  1.465716e-08         0.000000         0.000029        317,397,312   \n",
       "3  0.000000e+00         0.000089         0.000000  317,397,380,12627   \n",
       "4  0.000000e+00         0.000030         0.000000           317,4763   \n",
       "\n",
       "   gpt2-small-size        gpt2-medium  gpt2-medium-size         gpt2-large  \\\n",
       "0                2          317,45094                 2          317,45094   \n",
       "1                3       317,397,3099                 3       317,397,3099   \n",
       "2                3        317,397,312                 3        317,397,312   \n",
       "3                4  317,397,380,12627                 4  317,397,380,12627   \n",
       "4                2           317,4763                 2           317,4763   \n",
       "\n",
       "   gpt2-large-size facebook/opt-125m  facebook/opt-125m-size  \\\n",
       "0                2          83,26528                       2   \n",
       "1                3       83,873,1999                       3   \n",
       "2                3        83,873,808                       3   \n",
       "3                4  83,873,1069,8461                       4   \n",
       "4                2           83,2095                       2   \n",
       "\n",
       "  EleutherAI/gpt-neo-125M  EleutherAI/gpt-neo-125M-size  \n",
       "0               317,45094                             2  \n",
       "1            317,397,3099                             3  \n",
       "2             317,397,312                             3  \n",
       "3       317,397,380,12627                             4  \n",
       "4                317,4763                             2  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(NAMES_FILEPATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3861bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_prompt(prompt: str, \n",
    "                fillers: dict) -> str:\n",
    "    \"\"\"\n",
    "    Fills a prompt template by replacing placeholders with actual values.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt template containing placeholders in square brackets.\n",
    "        name (str): The name to insert into the prompt.\n",
    "        fillers (dict): Dictionary with possible replacements for placeholders.\n",
    "\n",
    "    Returns:\n",
    "        str: The filled prompt with all placeholders replaced.\n",
    "    \"\"\"\n",
    "    elems = re.findall(r\"\\[(.*?)\\]\", prompt)\n",
    "    for elem in elems:\n",
    "        if elem in fillers:\n",
    "            prompt = re.sub(rf\"\\[{elem}\\]\", random.choice(fillers[elem]), prompt)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e3d93e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(df: pd.DataFrame, prompt: str) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"original_prompt\": df.apply(lambda row: re.sub(r\"\\[name\\]\", row[\"name_1\"], prompt), axis=1),\n",
    "        \"corrupted_prompt\": df.apply(lambda row: re.sub(r\"\\[name\\]\", row[\"name_2\"], prompt), axis=1),\n",
    "        \"ablation_prompt\": df.apply(lambda row: re.sub(r\"\\[name\\]\", \"someone\", prompt), axis=1)\n",
    "    })\n",
    "\n",
    "def get_name_token_info(model: HookedTransformer, \n",
    "                        df: pd.DataFrame, \n",
    "                        name_col: str = \"name_1\", \n",
    "                        prompt_col: str = \"original_prompt\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns token ids, token strings, and token positions for just the name (not the whole prompt) using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model: The language model used for tokenization.\n",
    "        df: DataFrame containing at least the columns with names and prompts.\n",
    "        name_col: Name of the column containing the name. Default is \"name\".\n",
    "        prompt_col: Name of the column containing the prompt. Default is \"prompt\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns 'name', 'token_ids', 'token_strs', 'token_positions'.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        name = row[name_col]\n",
    "        prompt = row[prompt_col]\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        prompt_tokens = model.to_tokens(prompt, prepend_bos=True)[0].tolist()\n",
    "\n",
    "        # Tokenize the name\n",
    "        name_tokens = model.to_tokens(f\" {name}\", prepend_bos=False)[0].tolist()\n",
    "        name_token_strs = model.to_str_tokens(f\" {name}\", prepend_bos=False)\n",
    "        \n",
    "        # Find positions of name tokens in the prompt\n",
    "        positions = [ model.get_token_position(single_token=int(token), input=prompt, prepend_bos=True) for token in name_tokens ]\n",
    "\n",
    "        results.append({\n",
    "            \"subject_name\": name,\n",
    "            \"subject_token_ids\": \",\".join([str(t) for t in name_tokens]),\n",
    "            \"subject_token_strs\": \",\".join([str(t) for t in name_token_strs]),\n",
    "            \"subject_token_positions\": \",\".join([str(p) for p in positions]),\n",
    "            \"last_token_position\": len(prompt_tokens) - 1\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0d1bde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(names: pd.DataFrame,\n",
    "                   model: HookedTransformer,\n",
    "                   prompt: str,\n",
    "                   prompt_type: int,\n",
    "                   dataset_size: int = DATASET_SIZE,\n",
    "                   gdr_prop_thres: float = THRESHOLD,\n",
    "                   he_token: int = 339,\n",
    "                   she_token: int = 673\n",
    "                   ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a dataset by generating prompts and collecting model responses.\n",
    "\n",
    "    Args:\n",
    "        names: DataFrame containing names for prompt generation.\n",
    "        model: The model used for generating responses.\n",
    "        prompt: The prompt template to use for generation.\n",
    "        dataset_size: The desired size of the dataset.\n",
    "        template_type: The type of template to use.\n",
    "        gdr_prop_thres: The threshold for gender proportion in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: The generated dataset.\n",
    "    \"\"\"\n",
    "    males   = names[ names[\"M_weighted_norm\"] > names[\"F_weighted_norm\"] ]\n",
    "    females = names[ names[\"F_weighted_norm\"] > names[\"M_weighted_norm\"] ]\n",
    "\n",
    "    m_quantile = males[\"M_weighted_norm\"].quantile(gdr_prop_thres)\n",
    "    f_quantile = females[\"F_weighted_norm\"].quantile(gdr_prop_thres)\n",
    "    \n",
    "    males   = males[ males[\"M_weighted_norm\"] >= m_quantile ]\n",
    "    females = females[ females[\"F_weighted_norm\"] >= f_quantile ]\n",
    "\n",
    "    sampled_males   = males.sample(n=dataset_size // 2, weights=\"M_weighted_norm\", replace=False)\n",
    "    sampled_females = females.sample(n=dataset_size // 2, weights=\"F_weighted_norm\", replace=False)\n",
    "\n",
    "    data_1 = pd.concat([sampled_males, sampled_females], axis=0).reset_index(drop=True)\n",
    "    data_2 = pd.concat([sampled_females, sampled_males], axis=0).reset_index(drop=True)\n",
    "\n",
    "    data_1[\"expected_token_id\"] = he_token\n",
    "    data_2[\"expected_token_id\"] = she_token    \n",
    "\n",
    "    data_1 = data_1.add_suffix(\"_1\")\n",
    "    data_2 = data_2.add_suffix(\"_2\")\n",
    "\n",
    "    data = pd.concat([data_1, data_2], axis=1).reset_index(drop=True)\n",
    "\n",
    "    data = data.assign(**get_prompts(data, prompt))\n",
    "    data = data.assign(**get_name_token_info(model, data))\n",
    "    data[\"id\"] = range(1, len(data) + 1)\n",
    "    data[\"prompt_type\"] = prompt_type\n",
    "    data[\"expected_token_id\"] = data.apply(lambda row: he_token if row[\"M_weighted_norm_1\"] > row[\"F_weighted_norm_1\"] else she_token, axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b2df9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.DataFrame()\n",
    "\n",
    "with open(\"../../src/json/templates_he_she.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jdata = json.load(f)\n",
    "\n",
    "    df_filtered = df[ df[f\"{MODEL_NAME}-size\"] == TOKEN_NAME_SIZE ]\n",
    "    df_filtered = df_filtered[ [\"name\", \"F_weighted_norm\", \"M_weighted_norm\", f\"{MODEL_NAME}\", f\"{MODEL_NAME}-size\"] ]\n",
    "\n",
    "    for _ in range(PROMPT_TYPE_SIZE):\n",
    "        prompt      = random.choice(jdata[\"templates\"][TEMPLATE_TYPE][\"prompt_templates\"])\n",
    "        complements = jdata[\"complements\"]\n",
    "        prompt      = fill_prompt(prompt, complements)\n",
    "\n",
    "        data = create_dataset(names=df_filtered, \n",
    "                              model=model, \n",
    "                              prompt=prompt, \n",
    "                              prompt_type=_,\n",
    "                              he_token=339 if \"gpt\" in MODEL_NAME else 37,\n",
    "                              she_token=673 if \"gpt\" in MODEL_NAME else 79\n",
    "                              )\n",
    "\n",
    "        if len(dataset_df) == 0:\n",
    "            dataset_df = data\n",
    "        else:\n",
    "            dataset_df = pd.concat([dataset_df, data], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b45af366",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    \"id\": Value(\"int32\"),\n",
    "    \"prompt_type\": ClassLabel(names=[f\"type_{i}\" for i in range(PROMPT_TYPE_SIZE)]),\n",
    "    \"prompts\":{\n",
    "        \"org_prompt\": Value(\"string\"),\n",
    "        \"corr_prompt\": Value(\"string\"),\n",
    "        \"ablated_prompt\": Value(\"string\")\n",
    "    },\n",
    "    \"subject\":{\n",
    "        \"token_idxs\": Sequence(Value(\"int32\")),\n",
    "        \"tokens\": Sequence(Value(\"string\")),\n",
    "        \"pos\": Sequence(Value(\"int32\"))\n",
    "    },\n",
    "    \"end\":{\n",
    "        \"pos\": Value(\"int32\")\n",
    "    },\n",
    "    \"expected_token_id\": Value(\"int32\")\n",
    "})\n",
    "\n",
    "shared_config = {\n",
    "    \"F\": {\"token\": \" she\",\n",
    "          \"token_id\": she_token\n",
    "        },\n",
    "    \"M\": {\"token\": \" he\",\n",
    "          \"token_id\": he_token\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4fecf1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    \"id\": dataset_df[\"id\"].tolist(),\n",
    "    \"prompt_type\": dataset_df[\"prompt_type\"].tolist(),\n",
    "    \"prompts\":[\n",
    "        {\n",
    "            \"org_prompt\": org,\n",
    "            \"corr_prompt\": corr,\n",
    "            \"ablated_prompt\": ablated\n",
    "        } for org, corr, ablated in zip(dataset_df[\"original_prompt\"].tolist(),\n",
    "                                       dataset_df[\"corrupted_prompt\"].tolist(),\n",
    "                                       dataset_df[\"ablation_prompt\"].tolist())\n",
    "    ],\n",
    "    \"subject\":[\n",
    "        {\n",
    "            \"token_idxs\": [int(t) for t in token_ids.split(\",\")],\n",
    "            \"tokens\": [str(t) for t in token_strs.split(\",\")],\n",
    "            \"pos\": [int(p) for p in positions.split(\",\")]\n",
    "        } for token_ids, token_strs, positions in zip(dataset_df[\"subject_token_ids\"].tolist(),\n",
    "                                                     dataset_df[\"subject_token_strs\"].tolist(),\n",
    "                                                     dataset_df[\"subject_token_positions\"].tolist())\n",
    "    ],\n",
    "    \"end\":[\n",
    "        {\"pos\": pos} for pos in dataset_df[\"last_token_position\"].tolist()\n",
    "    ],\n",
    "    \"expected_token_id\": dataset_df[\"expected_token_id\"].tolist()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d01a6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = Dataset.from_dict(dataset_dict, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "94e4e4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In a moment of clarity, George mentioned how',\n",
       " 'As people say, Sam confessed when',\n",
       " \"If I'm not mistaken, Eric (with a smile) decided that\",\n",
       " 'In a moment of clarity, Jose (with excitement) decided how',\n",
       " 'As people say, Jason acknowledged that',\n",
       " 'Back then, Cameron confessed in a playful voice how',\n",
       " 'At that moment, Richard (in a clear voice) admitted why',\n",
       " 'In a moment of clarity, Jose (with a smile) mentioned that',\n",
       " \"From what I've heard, Ryan said with hesitation when\",\n",
       " 'As I recall, Jose (to the masses) mentioned that',\n",
       " 'During the event, Matthew revealed what',\n",
       " 'At that moment, Jonathan mentioned to the assembly how',\n",
       " 'Before anyone noticed, Justin announced how',\n",
       " 'At the party, Julian declared in a loud voice that',\n",
       " 'During the event, Robert admitted despite the noise what',\n",
       " 'A few days back, Adrian announced with excitement when',\n",
       " 'A few days back, Phillip (in a hesitant voice) announced that',\n",
       " 'Last time Terry confessed in a nervous voice that',\n",
       " 'In a flash of insight, Nicole decided in a trembling voice that',\n",
       " 'In a moment of clarity, Diane (with determination) said that']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[\"prompts\"][\"org_prompt\"][[ idx for idx in range(1, 3000, 150)]] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a1b4a6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 11,\n",
       " 'prompt_type': 2,\n",
       " 'prompts': {'org_prompt': 'As people say, Kevin confessed when',\n",
       "  'corr_prompt': 'As people say, Helen confessed when',\n",
       "  'ablated_prompt': 'As people say, someone confessed when'},\n",
       " 'subject': {'token_idxs': [7939], 'tokens': [' Kevin'], 'pos': [5]},\n",
       " 'end': {'pos': 7},\n",
       " 'expected_token_id': 339}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset[158]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a0135025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3700/3700 [00:00<00:00, 192690.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "final_dataset.save_to_disk(f\"../../datasets/{TEMPLATE_TYPE}_{TOKEN_NAME_SIZE}_tokens{'_opt' if 'opt' in MODEL_NAME else ''}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
